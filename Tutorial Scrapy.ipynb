{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy startproject tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos: <br>\n",
    "1) En carpeta 'spiders' crear fichero trabajandoTodo.py <br>\n",
    "2) identificar url a analizar <br>\n",
    "3) identificar el link de cada ficha <br>\n",
    "4) entrar a cada ficha y extraer la información <br>\n",
    "5) continuar con la extracción en la siguiente página <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer información de la primera página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# scrapy crawl trabajandoTodo -o trabajandoTodo.csv -t csv -a CSV_DELIMITER=\"\\t\"\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class AnalisisHorizontalSpider(scrapy.Spider):\n",
    "    name = \"trabajando\"\n",
    "    # URL a analizar\n",
    "    start_urls = ['https://www.trabajando.cl/jobs/home/']\n",
    "\n",
    "    # Campos que quiero extraer\n",
    "    def parse(self, response):\n",
    "        for oferta in response.css('li.oferta_item'):\n",
    "            yield {\n",
    "                'link': oferta.css('h2.elcargo a::attr(href)').extract_first(),\n",
    "                'text': oferta.css('div.sub_title div.t2::text').extract_first(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer información de la todas las páginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# scrapy crawl trabajandoTodo -o trabajandoTodo.csv -t csv -a CSV_DELIMITER=\"\\t\"\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class AnalisisHorizontalSpider(scrapy.Spider):\n",
    "    name = \"trabajando\"\n",
    "    # URL a analizar\n",
    "    start_urls = ['https://www.trabajando.cl/jobs/home/']\n",
    "\n",
    "    # Campos que quiero extraer\n",
    "    def parse(self, response):\n",
    "        for oferta in response.css('li.oferta_item'):\n",
    "            yield {\n",
    "                'link': oferta.css('h2.elcargo a::attr(href)').extract_first(),\n",
    "                'text': oferta.css('div.sub_title div.t2::text').extract_first(),\n",
    "            }\n",
    "\n",
    "        # Siguiente Pagina\n",
    "        for href in response.xpath('//*[@id=\"nextPageEmpresa\"]/@href'):\n",
    "            yield response.follow(href, self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer información de la todas fichas y de todas las páginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# scrapy crawl trabajandoTodo -o trabajandoTodo.csv -t csv -a CSV_DELIMITER=\"\\t\"\n",
    "import scrapy\n",
    "import re\n",
    "\n",
    "\n",
    "class AnalisisHorizontalSpider(scrapy.Spider):\n",
    "    name = 'trabajandoTodo'\n",
    "\n",
    "    start_urls = ['https://www.trabajando.cl/jobs/home/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # SIGUIENDO LA FICHA DE CADA OFERTA\n",
    "        for href in response.css('h2.elcargo a::attr(href)').extract():\n",
    "            yield response.follow(href, self.parse_author)\n",
    "\n",
    "        # SIGUIENTE PAGINA\n",
    "        for href in response.xpath('//*[@id=\"nextPageEmpresa\"]/@href'):\n",
    "            yield response.follow(href, self.parse)\n",
    "\n",
    "        # CONTENIDO DE LA FICHA DE OFERTA\n",
    "    def parse_author(self, response):\n",
    "        def extract_with_css(query):\n",
    "            return response.css(query).extract_first().strip()\n",
    "\n",
    "        def extract_with_xpath(query):\n",
    "            return response.xpath(query).extract_first().strip()\n",
    "\n",
    "        yield {\n",
    "            'Titulo'        : extract_with_css('h1.offerTitleText span::text'),\n",
    "            'nombre_empresa': extract_with_xpath('//*[@id=\"detalle_oferta\"]/div[1]/div[3]/div[1]/div/div[1]/h4/text()'),\n",
    "            'Categoria'     : extract_with_css('div.datos-empresa h3.categoria_empresa::text'),\n",
    "            'Cargo'         : extract_with_xpath('//*[@id=\"detalle_oferta\"]/div[2]/div[3]/div[1]/div[2]/text()'),\n",
    "            'Vacantes'      : response.css('div.col-md-8.txt::text')[1].extract().strip(),\n",
    "            'Sueldo'        : response.css('div.col-md-8.txt::text')[10].extract().strip(),\n",
    "            'Publicado'     : extract_with_xpath('//*[@id=\"detalle_oferta\"]/div[1]/div[2]/ul/li[1]/h4/text()'),\n",
    "            'Finaliza'      : extract_with_xpath('//*[@id=\"detalle_oferta\"]/div[1]/div[2]/ul/li[2]/h4/text()'),\n",
    "            'url'           : response.url,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar desde consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl trabajandoTodo -o trabajandoTodo.csv -t csv -a CSV_DELIMITER=\"\\t\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
